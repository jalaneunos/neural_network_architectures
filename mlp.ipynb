{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoKci5y2fYk2nnq0+lmdp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalaneunos/neural_network_architectures/blob/main/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "q1gqaBGblrll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 layer ANN without PyTorch nn modules"
      ],
      "metadata": {
        "id": "SGPKECZlmACc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation functions"
      ],
      "metadata": {
        "id": "I_NImSBYswC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return torch.maximum(torch.tensor(0.0), x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "54psrLjtrLjY"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self, num_neurons, input_dimensions):\n",
        "    self.weights = torch.rand((input_dimensions, num_neurons), dtype=torch.float64)\n",
        "    self.bias = torch.zeros(num_neurons)\n",
        "    self.most_recent_input = None # used to calculate backprop\n",
        "    self.most_recent_u = None # before activation function\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "  def __init__(self, num_neurons, input_dimensions):\n",
        "    super().__init__(num_neurons, input_dimensions)\n",
        "    self.activation = lambda x: x\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.most_recent_input = x\n",
        "    u = torch.transpose(self.weights, 0, 1) @ x + self.bias\n",
        "    y = self.activation(u)\n",
        "    return y\n",
        "\n",
        "  def derivative_activation(self, x): return 1\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Linear Layer with {len(self.bias)} neurons\"\n",
        "\n",
        "\n",
        "class ReLULayer(Layer):\n",
        "  def __init__(self, num_neurons, input_dimensions):\n",
        "    super().__init__(num_neurons, input_dimensions)\n",
        "    self.activation = relu\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.most_recent_input = x\n",
        "    u = torch.transpose(self.weights, 0, 1) @ x + self.bias\n",
        "    self.most_recent_u = u\n",
        "    y = self.activation(u)\n",
        "    return y\n",
        "\n",
        "  def derivative_activation(self, x): return torch.gt(x, 0).type(x.dtype)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"ReLU Layer with {len(self.bias)} neurons\"\n",
        "\n",
        "\n",
        "\n",
        "class ANN:\n",
        "  def __init__(self, num_of_layers, input_dimensions, output_dimensions, layer_width, lr=0.001):\n",
        "    self.layers = []\n",
        "    for i in range(num_of_layers):\n",
        "      if i == 0:\n",
        "        layer = ReLULayer(layer_width, input_dimensions)\n",
        "      elif i == num_of_layers - 1: # output layer\n",
        "        layer = LinearLayer(output_dimensions, layer_width)\n",
        "      else:\n",
        "        layer = ReLULayer(layer_width, layer_width)\n",
        "      self.layers.append(layer)\n",
        "    self.lr = lr\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  def loss(self, d, y):\n",
        "    return -(d - y)\n",
        "\n",
        "  def backward(self, d, y):\n",
        "    loss = self.loss(d, y)\n",
        "\n",
        "    out = self.layers[-1]\n",
        "    hidden = self.layers[1]\n",
        "    hidden_loss = out.weights @ loss * hidden.derivative_activation(hidden.most_recent_u)\n",
        "\n",
        "    out.bias -= self.lr * loss\n",
        "    out.weights -= self.lr * loss * out.most_recent_input.view(-1, 1)\n",
        "\n",
        "    hidden.bias -= self.lr * hidden_loss\n",
        "    hidden.weights -= self.lr * hidden.most_recent_input @ hidden_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HHuSZsgLl32m"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann = ANN(3, 3, 1, 4)"
      ],
      "metadata": {
        "id": "13tpVuGFl6-c"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 3], dtype=torch.float64)"
      ],
      "metadata": {
        "id": "1xsTOO-go6TB"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = ann.forward(x)"
      ],
      "metadata": {
        "id": "BDXqsDoRpA1Q"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann.layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hde86ARBpUzs",
        "outputId": "be901a58-3fc9-4525-becb-6f2449809925"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ReLU Layer with 4 neurons,\n",
              " ReLU Layer with 4 neurons,\n",
              " Linear Layer with 1 neurons]"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([\n",
        "    [1.5, 2.3, 0.7],\n",
        "    [0.6, 1.1, 3.4],\n",
        "    [2.2, 0.5, 1.1],\n",
        "    [3.3, 2.1, 0.9],\n",
        "    [1.0, 1.5, 2.0],\n",
        "    [2.5, 0.3, 2.2],\n",
        "    [1.8, 2.4, 1.5],\n",
        "    [0.9, 0.8, 2.3]\n",
        "], dtype=torch.float64)\n",
        "\n",
        "D = torch.tensor([4.2, 2.8, 3.5, 5.7, 3.0, 3.8, 4.1, 2.5],  dtype=torch.float64)"
      ],
      "metadata": {
        "id": "Bo4I9VyrtYEH"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the neural network"
      ],
      "metadata": {
        "id": "JbLMa_9Eo72Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "  mse = 0\n",
        "  for i in range(len(X)):\n",
        "    x, d = X[i], D[i]\n",
        "    y = ann.forward(x)\n",
        "    mse += ann.loss(d, y) ** 2\n",
        "    ann.backward(d, y)\n",
        "  print(f'Epoch {epoch}: MSE: {mse / len(X)}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh2F-kcncefP",
        "outputId": "7cb9c54f-b86f-463d-bf0f-35407dda7ff0"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: MSE: tensor([8.9002], dtype=torch.float64)\n",
            "Epoch 1: MSE: tensor([1.5796], dtype=torch.float64)\n",
            "Epoch 2: MSE: tensor([1.2452], dtype=torch.float64)\n",
            "Epoch 3: MSE: tensor([1.2212], dtype=torch.float64)\n",
            "Epoch 4: MSE: tensor([1.2207], dtype=torch.float64)\n",
            "Epoch 5: MSE: tensor([1.2210], dtype=torch.float64)\n",
            "Epoch 6: MSE: tensor([1.2207], dtype=torch.float64)\n",
            "Epoch 7: MSE: tensor([1.2202], dtype=torch.float64)\n",
            "Epoch 8: MSE: tensor([1.2195], dtype=torch.float64)\n",
            "Epoch 9: MSE: tensor([1.2189], dtype=torch.float64)\n",
            "Epoch 10: MSE: tensor([1.2182], dtype=torch.float64)\n",
            "Epoch 11: MSE: tensor([1.2175], dtype=torch.float64)\n",
            "Epoch 12: MSE: tensor([1.2168], dtype=torch.float64)\n",
            "Epoch 13: MSE: tensor([1.2162], dtype=torch.float64)\n",
            "Epoch 14: MSE: tensor([1.2155], dtype=torch.float64)\n",
            "Epoch 15: MSE: tensor([1.2148], dtype=torch.float64)\n",
            "Epoch 16: MSE: tensor([1.2141], dtype=torch.float64)\n",
            "Epoch 17: MSE: tensor([1.2135], dtype=torch.float64)\n",
            "Epoch 18: MSE: tensor([1.2128], dtype=torch.float64)\n",
            "Epoch 19: MSE: tensor([1.2122], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAN-UXXncoh9"
      },
      "execution_count": 195,
      "outputs": []
    }
  ]
}